# pyspark_crash_code
This repository contains a file of basic but essential code functions that can be used in PySpark, a Python library for distributed data processing. These functions are designed to help you accelerate your data processing tasks and take advantage of the distributed computing capabilities of Apache Spark.

**Installation**
All the code was done in google colab, as such to use pyspark the env needed to be configured. All code for configuration has been included under the **Import** section, thus allowing you to confugure google colab for Apache spark and pyspark. To run simply clone the directory and run the cells in order.
**clone:** git clone https://github.com/your-username/pyspark-mini-file.git

**Usage**
It is just one file outlining -> [import, data set importing, inspection of df, null duplication handeling, advanced filtering, sql on dataframes, adding calc columns with conditions, saving a file, aggregations & Alias]
Feel free to import your own dataset and adjust column fields accordingly.
This is something that i shall be adding to more and more as i find neeche little ways to solve problems that i face.

**License**
This project is licensed under the MIT License. You are free to use, modify, and distribute the code in this repository for both commercial and non-commercial purposes.

Happy data processing with PySpark!
